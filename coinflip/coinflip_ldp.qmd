---
title: "Bayesian inference for a sequence of coinflips"
author: "Jakob Hoffmann"
date: "01/01/2023"
format: html
jupyter: julia-1.8
---


## Setup
We're interested in performing inference on a simple process where a
possibly biased coin is flipped $N=100$ times. We start by simulating
data from a Bernoulli distribution with the probability of heads set
to $p = 0.7$,  which for the inverse problem is going to be the unknown
quantity of interest to be inferred from observed data.

```{julia}
using Distributions
```


```{julia}
N = 100
p = 0.7
d = Bernoulli(p)
data = rand(d, N);
```

## Model definition

Having simulated data for inference, we now proceed to the model definition using
the `LogDensityProblems` interface package.
As we assume that the individual coinflips are independent from one another,
the exact sequence of flips is irrelevant and we just need to store the total
number of flips and the number of heads, which we call $y$. We store this
information in a struct called `CoinflipProblem`, for which we also create a
constructor that extracts the necessary information from a sequence of flips.

```{julia}
struct CoinflipProblem
  N::Int
  y::Int
end

function CoinflipProblem(data::AbstractVector{Bool})
  N, y = length(data), sum(data)
  CoinflipProblem(N, y)
end;
```
We now make our problem struct callable on an input parameter $\theta$
at which to evaluate the (unnormalized) log joint probability density of the posterior distribution.
Next to the likelihood function which makes use of the information from the data, we also
need to specify a prior distribution for the unknown quantities. Here, we're going to be broadly 
skeptical of extremely biased coins and use a $Beta(2,2)$ prior:

```{julia}
using CairoMakie

plot(Beta(2,2))
```


```{julia}
function (problem::CoinflipProblem)(theta)
  (; N, y) = problem
  (; p) = theta
  logprior = logpdf(Beta(2,2), p)
  loglikelihood = logpdf(Binomial(N, p), y)
  logprior + loglikelihood
end  
```

We can now instantiate our problem on the data and evaluate the joint logdensity of a fair
coinflip with $p = 0.5$:

```{julia}
problem = CoinflipProblem(data)
problem((; p=0.5))
```

## Model estimation
Having defined a way to evaluate the posterior density for a given parameter value,
we now proceed to set up a sampling-based numeric estimation procedure via Hamiltonian 

Monte Carlo (HMC) using the `LogDensityProblems` suite of packages. HMC operates on the
unconstrained reals but our parameter $p$ is confined to the unit interval $(0,1)$ so we
need an appropriate transformation, which is conveniently available in the 
`TransformedLogDensity` package. HMC furthermore requires the gradient of the posterior 
density, which we get with an automatic differentiation package, in this case
`ForwardDiff`.

```{julia}
using LogDensityProblems
using TransformVariables, TransformedLogDensities
using LogDensityProblemsAD, ForwardDiff

tran = as((p=as_unit_interval,))
p = TransformedLogDensity(tran, problem)
grad = ADgradient(:ForwardDiff, p)
```

We can now evaluate the logdensity and its gradient:


```{julia}
LogDensityProblems.logdensity_and_gradient(grad, zeros(1))
```

With this in place, we can now draw a large number of samples (say, $S=2000$) from the posterior 
distribution using the HMC implementation in `DynamicHMC` as an approximation. We use the `ThreadsX`
package to sample $k$ chains in parallel:

```{julia}
using Random
using DynamicHMC
using ThreadsX

function sample(grad, S, k; rng=Random.default_rng()) 
   ThreadsX.map(1:k) do _
     mcmc_with_warmup(rng, grad, S; reporter=NoProgressReport())
   end
end

result = sample(grad, 2000, 4)
```

The `result` is a vector of length $k$, each element of which contains for each chain  the posterior samples
as well as some statistics about the sampling procedure, which can be used to check if everything went as planned.

## Model checking

Having obtained samples from the posterior distribution, we're in principle ready to 
use our model for inference, i.e., answer the question of whether our coin is biased
and by how much, and how certain we can be of the answer based on the data we have seen.

However, before we jump to inference, it is good practice to perform some model checks: 
Our estimates rely on a numerical sampling scheme, which can fail rendering the results
unreliable. 

```{julia}
using MCMCDiagnosticTools
using DynamicHMC.Diagnostics
```

First, we chan check the effective sample size (ess). In Markov chain monte carlo (MCMC) approaches,
samples are often correlated, meaning that the total number of 'effective' samples is less than
obtained by an uncorrelated sampling procedure.

```{julia}
ess, Rhat =  ess_rhat(stack_posterior_matrices(result))
```

```{julia}
summarize_tree_statistics.(getfield.(result, :tree_statistics))
```

## Model inference

```{julia}
using StructArrays

function posterior(result)
  samples = eachcol(pool_posterior_matrices(result))
  StructArray(transform.(tran, samples))
end

post = posterior(result);
```


```{julia}
function summarize(post)
  m, s = round.((mean(post.p), std(post.p)); digits=2)
  println("posterior mean: ", m)
  println("posterior sd: ", s)
end

summarize(post)
```
